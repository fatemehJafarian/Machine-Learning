{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPliFJsiBJnZEOuS5VARfpv"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NyuIVisLc8sW","executionInfo":{"status":"ok","timestamp":1606002275337,"user_tz":-210,"elapsed":1109,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"50c12cd4-f851-4aed-d332-cf3e8e3af3df"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ilJIYw0kBLb"},"source":["import pandas as pd\n","import numpy as np\n","import random\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnEoRSRpS64N"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics \n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDBNsWpQkERd"},"source":["data = pd.read_csv('/content/drive/My Drive/Test/HW2/2/prison_dataset.csv', header = None, skiprows = 1)\n","labels = (pd.read_csv('/content/drive/My Drive/Test/HW2/2/prison_dataset.csv', header = None, nrows = 1)).to_numpy()\n","data_np = data.to_numpy()\n","dim = data_np.shape\n","rows_num = 4*dim[0]//5\n","indx = random.sample(range(dim[0]), rows_num)\n","train_data = []\n","test_data = np.delete(data_np, indx, axis = 0)\n","for i in range(rows_num):\n","  train_data.append(data_np[indx[i]])\n","train_data = np.array(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZztDrsO1zSU"},"source":["def Entropy(input, labels):\n","  count = []\n","  for label in labels:\n","    count.append(np.count_nonzero(np.transpose(input) == label))\n","  E = 0\n","  dim = input.shape\n","  if dim[0] is 0:\n","    return E\n","  count[:] = [x / dim[0] for x in count]\n","  for c in count:\n","    if (c != 0):\n","      E -= c*math.log2(c)\n","  return E"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrx9gC1hTKdX"},"source":["def Gain(S_entropy, p, entropies):\n","  g = (np.matmul(entropies, np.transpose(p))).diagonal()\n","  return np.argmin(g)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfIHrwlOzgvK"},"source":["def ID3(S, S_labels, features, features_labels):\n","  E = Entropy(S, S_labels)\n","  dim0 = len(features_labels)\n","  dim1 = 0\n","  for i in features_labels:\n","    dim1 = max(dim1, len(i))\n","  entropies = np.zeros((dim0, dim1))\n","  count = np.zeros((dim0, dim1))\n","  for i in range(dim0):\n","    for j in range(len(features_labels[i])):\n","      indxs = (np.where(features[:, i] == features_labels[i][j]))[0]\n","      entropies[i][j] = Entropy(np.take(S, indxs), S_labels)\n","      count[i][j] = len(indxs)\n","  indx = Gain(E, count, entropies)\n","  node_indxs = []\n","  for i in range(len(features_labels[indx])):\n","    node_indxs.append((np.where(features[:, indx] == features_labels[indx][i]))[0])\n","  return indx, node_indxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wSw5BVKbOAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605981598763,"user_tz":-210,"elapsed":1024,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"2c70862c-f1cb-4395-f39d-4cda23c4c45c"},"source":["S_labels = np.unique(np.transpose(train_data[:, 10]))\n","features_labels = []\n","for i in range(10):\n","  features_labels.append(np.unique(np.transpose(train_data[:,i])))\n","indx1, node1 = ID3(train_data[:, 10], S_labels, train_data[:, 0:10], features_labels)\n","\n","print(labels[0][indx1])\n","print(features_labels[indx1])\n","features_labels = np.delete(features_labels, indx1, axis = 0)\n","labels = np.delete(labels, indx1, axis = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fiscal Year Released\n","[2010 2013 2015]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYc4ZG6aErlD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605981602002,"user_tz":-210,"elapsed":1226,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"973e0707-c46c-4f7f-94a1-e53e4739e5d4"},"source":["help2 = np.delete(train_data, indx1, axis = 1)\n","indx2 = [None]*len(node1)\n","node2 = [None]*len(node1)\n","for i in range(len(node1)):\n","  indx2[i], node2[i] = ID3(train_data[node1[i], 10], S_labels, help2[node1[i], 0:9], features_labels)\n","\n","for i in range(len(indx2)):\n","  print(labels[0][indx2[i]])\n","  print(features_labels[indx2[i]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Recidivism Reporting Year\n","[2013 2016 2018]\n","Age At Release\n","['<45' '>45']\n","Release Type\n","['Discharged End of Sentence' 'Parole']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"se6dYu49pLzq","executionInfo":{"status":"ok","timestamp":1605982564693,"user_tz":-210,"elapsed":1051,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"5fceec5b-0015-4747-8b20-e8fa95578c0f"},"source":["leaf_states = []\n","for i in range(3):\n","  train = train_data[node1[i], :]\n","  for j in range(len(node2[i])):\n","    o1 = np.where((train[node2[i][j], 10] == 0))\n","    o2 = np.where((train[node2[i][j], 10] == 1))\n","    if (len(o1[0]) > len(o2[0])):\n","      leaf_states.append(0)\n","    elif len(o2[0]) != 0:\n","      leaf_states.append(1)\n","print(leaf_states)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1, 1, 1, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XlO38dZclt0g"},"source":["The state of every leaf is determined by cheking the majority of the data the has the specified value."]},{"cell_type":"code","metadata":{"id":"29YSe7q_AMrH"},"source":["def Test_ID3(test_data):\n","  confusion_matrix = np.zeros((2, 2))\n","  no_data = 0\n","  for i in range((test_data.shape)[0]):\n","    if ((test_data[i, 0] == 2010 and test_data[i, 1] == 2013) or (test_data[i, 0] == 2013)):\n","      if (test_data[i, 10] == 1):\n","        confusion_matrix[1, 1] += 1\n","      else:\n","        confusion_matrix[0, 1] += 1\n","    elif (test_data[i, 0] == 2015):\n","      if (test_data[i, 10] == 0):\n","        confusion_matrix[0, 0] += 1\n","      else:\n","        confusion_matrix[1, 0] += 1\n","    else:\n","      no_data += 1\n","  return confusion_matrix, no_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLK3JKGi_3cW","executionInfo":{"status":"ok","timestamp":1605985642687,"user_tz":-210,"elapsed":1957,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"5448b2f1-7ba5-438b-c221-7e17b9ab9e25"},"source":["Confusion_Matrix, no_data = Test_ID3(test_data)\n","print('Confusion Matrix:\\n', Confusion_Matrix)\n","accuracy = (Confusion_Matrix[0, 0]+Confusion_Matrix[1, 1])/(sum(sum(Confusion_Matrix)))\n","print('Accuracy is:  ', accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Confusion Matrix:\n"," [[1188.  208.]\n"," [ 650. 1039.]]\n","Accuracy is:   0.7218800648298217\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"71pw674dmvaO"},"source":["def Random_Forest(data, labels, features_labels, labels_name):\n","  p = 5\n","  indxs = random.sample(range(10), p)\n","  f = []\n","  for i in range(p):\n","    f.append(features_labels[indxs[i]])\n","  indx1, node1 = ID3(data[:, 10], labels, data[:, indxs], f)\n","  print(labels_name[0][indxs[indx1]])\n","  print(f[indx1])\n","  features_labels = np.delete(features_labels, indx1, axis = 0)\n","  labels_name = np.delete(labels_name, indx1, axis = 1)\n","\n","  indx2 = [None]*len(node1)\n","  node2 = [None]*len(node1)\n","  for i in range(len(node1)):\n","    help2 = (np.delete(data, indx1, axis = 1))[node1[i]]\n","    indxs = random.sample(range(8), p)\n","    f = []\n","    for j in range(p):\n","      f.append(features_labels[indxs[j]])\n","    indx2[i], node2[i] = ID3(data[node1[i], 10], labels, help2[:, indxs], f)\n","\n","  for i in range(len(indx2)):\n","    print(labels_name[0][indxs[indx2[i]]])\n","    print(f[indx2[i]])\n","  \n","  leaf_states = []\n","  for i in range(len(node1)):\n","    d = data[node1[i], :]\n","    for j in range(len(node2[i])):\n","      o1 = np.where((data[node2[i][j], 10] == 0))\n","      o2 = np.where((data[node2[i][j], 10] == 1))\n","      if (len(o1[0]) > len(o2[0])):\n","        leaf_states.append(0)\n","      elif len(o2[0]) != 0:\n","        leaf_states.append(1)\n","  print(leaf_states)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_03LfzqwZXsc"},"source":["train_data_rf = np.array_split(train_data, 4)\n","\n","S_labels = np.unique(np.transpose(train_data[:, 10]))\n","features_labels = []\n","for i in range(10):\n","  features_labels.append(np.unique(np.transpose(train_data[:,i])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_1U8z-zygOh","executionInfo":{"status":"ok","timestamp":1606005698877,"user_tz":-210,"elapsed":1219,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"90bf3ceb-4135-4531-da31-b0831f4a8dc3"},"source":["l = labels\n","f = features_labels\n","for i in range(4):\n","  Random_Forest(train_data_rf[i], S_labels, f, l)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Recidivism Reporting Year\n","[2013 2016 2018]\n","Convicting Offense Classification\n","['D Felony' 'Other Felony']\n","Age At Release\n","['<45' '>45']\n","Convicting Offense Classification\n","['D Felony' 'Other Felony']\n","[1, 1, 1, 1, 1, 1, 1]\n","Part of Target Population\n","['No' 'Yes']\n","Convicting Offense Subtype\n","['Other' 'Trafficking']\n","Recidivism Reporting Year\n","[2013 2016 2018]\n","[1, 1, 1, 1, 1, 1]\n","Convicting Offense Type\n","['Drug' 'Other' 'Violent']\n","Convicting Offense Type\n","['Drug' 'Other' 'Violent']\n","Convicting Offense Type\n","['Drug' 'Other' 'Violent']\n","Age At Release\n","['<45' '>45']\n","[1, 1, 1, 1, 1, 1, 1, 1]\n","Convicting Offense Type\n","['Drug' 'Other' 'Violent']\n","Convicting Offense Subtype\n","['Other' 'Trafficking']\n","Recidivism Reporting Year\n","[2013 2016 2018]\n","Recidivism Reporting Year\n","[2013 2016 2018]\n","[1, 1, 1, 1, 1, 1, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0voQka0_mFxk"},"source":["As you can see all of the leeves have the same label in all trees. I tried other ways too, but I think the problem is that variance of the given data is low and for getting better results we should train more trees and this code take so much time to handle the data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"XuxthLp7sXNc","executionInfo":{"status":"ok","timestamp":1606003499341,"user_tz":-210,"elapsed":1141,"user":{"displayName":"Fateme Jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgA-ReIY4igjyVC5-zt3bF8loUMbKqpQoym-iin=s64","userId":"17072147449478534346"}},"outputId":"ff45ea1c-39b6-42af-abad-91e1e85b6b31"},"source":["df = pd.read_csv('/content/drive/My Drive/Test/HW2/2/prison_dataset.csv')\n","df = df.apply(LabelEncoder().fit_transform)\n","X = df.drop('Recidivism - Return to Prison numeric', axis = 1)\n","y = df['Recidivism - Return to Prison numeric']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n","random_forest = RandomForestClassifier(n_estimators=30, max_depth=3, random_state=0)\n","random_forest.fit(X_train, y_train)\n","y_predict = random_forest.predict(X_test)\n","\n","print('Accuracy is:\\n', accuracy_score(y_test, y_predict))\n","pd.DataFrame(confusion_matrix(y_test, y_predict),columns=['Predicted 0', 'Predicted 1'],index=['True 0', 'True 1'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy is:\n"," 0.7277147487844409\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Predicted 0</th>\n","      <th>Predicted 1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>True 0</th>\n","      <td>1084</td>\n","      <td>217</td>\n","    </tr>\n","    <tr>\n","      <th>True 1</th>\n","      <td>623</td>\n","      <td>1161</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Predicted 0  Predicted 1\n","True 0         1084          217\n","True 1          623         1161"]},"metadata":{"tags":[]},"execution_count":305}]},{"cell_type":"markdown","metadata":{"id":"c8HVQsJbmxGB"},"source":["As we can see in this aproach we get bettter accuracy, because in this part we used the libraris of Python and they are so fast. So by training more trees we can get better results. "]}]}